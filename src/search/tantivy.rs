use std::path::Path;
use std::sync::atomic::{AtomicI64, Ordering};
use std::time::{SystemTime, UNIX_EPOCH};

use anyhow::{Result, anyhow};
use tantivy::schema::{
    FAST, Field, INDEXED, IndexRecordOption, STORED, STRING, Schema, TEXT, TextFieldIndexing,
    TextOptions,
};
use tantivy::{Index, IndexReader, IndexWriter, doc};
use tracing::{debug, info, warn};

use crate::connectors::NormalizedConversation;

const SCHEMA_VERSION: &str = "v4";

/// Minimum time (ms) between merge operations
const MERGE_COOLDOWN_MS: i64 = 300_000; // 5 minutes

/// Segment count threshold above which merge is triggered
const MERGE_SEGMENT_THRESHOLD: usize = 4;

/// Global last merge timestamp (ms since epoch)
static LAST_MERGE_TS: AtomicI64 = AtomicI64::new(0);

/// Debug status for segment merge operations
#[derive(Debug, Clone)]
pub struct MergeStatus {
    /// Current number of searchable segments
    pub segment_count: usize,
    /// Timestamp of last merge (ms since epoch), 0 if never
    pub last_merge_ts: i64,
    /// Milliseconds since last merge, -1 if never merged
    pub ms_since_last_merge: i64,
    /// Segment count threshold for auto-merge
    pub merge_threshold: usize,
    /// Cooldown period between merges (ms)
    pub cooldown_ms: i64,
}

impl MergeStatus {
    /// Returns true if merge is recommended based on current status
    pub fn should_merge(&self) -> bool {
        self.segment_count >= self.merge_threshold
            && (self.ms_since_last_merge < 0 || self.ms_since_last_merge >= self.cooldown_ms)
    }
}

// Bump this when schema/tokenizer changes. Used to trigger rebuilds.
pub const SCHEMA_HASH: &str = "tantivy-schema-v4-edge-ngram-agent-string";

#[derive(Clone, Copy)]
pub struct Fields {
    pub agent: Field,
    pub workspace: Field,
    pub source_path: Field,
    pub msg_idx: Field,
    pub created_at: Field,
    pub title: Field,
    pub content: Field,
    pub title_prefix: Field,
    pub content_prefix: Field,
    pub preview: Field,
}

pub struct TantivyIndex {
    pub index: Index,
    writer: IndexWriter,
    pub fields: Fields,
}

impl TantivyIndex {
    pub fn open_or_create(path: &Path) -> Result<Self> {
        let schema = build_schema();
        std::fs::create_dir_all(path)?;

        let meta_path = path.join("schema_hash.json");
        let mut needs_rebuild = true;
        if meta_path.exists() {
            let meta = std::fs::read_to_string(&meta_path)?;
            if meta.contains(SCHEMA_HASH) {
                needs_rebuild = false;
            }
        }

        if needs_rebuild {
            // Recreate index directory completely to avoid stale lock files.
            let _ = std::fs::remove_dir_all(path);
            std::fs::create_dir_all(path)?;
        }

        let mut index = if path.join("meta.json").exists() && !needs_rebuild {
            Index::open_in_dir(path)?
        } else {
            Index::create_in_dir(path, schema.clone())?
        };

        ensure_tokenizer(&mut index);

        std::fs::write(&meta_path, format!("{{\"schema_hash\":\"{SCHEMA_HASH}\"}}"))?;

        let writer = index
            .writer(50_000_000)
            .map_err(|e| anyhow!("create index writer: {e:?}"))?;
        let fields = fields_from_schema(&schema)?;
        Ok(Self {
            index,
            writer,
            fields,
        })
    }

    pub fn add_conversation(&mut self, conv: &NormalizedConversation) -> Result<()> {
        self.add_messages(conv, &conv.messages)
    }

    pub fn delete_all(&mut self) -> Result<()> {
        self.writer.delete_all_documents()?;
        Ok(())
    }

    pub fn commit(&mut self) -> Result<()> {
        self.writer.commit()?;
        Ok(())
    }

    pub fn reader(&self) -> Result<IndexReader> {
        Ok(self.index.reader()?)
    }

    /// Get current number of searchable segments
    pub fn segment_count(&self) -> usize {
        self.index
            .searchable_segment_ids()
            .map(|ids| ids.len())
            .unwrap_or(0)
    }

    /// Returns debug info about merge status
    pub fn merge_status(&self) -> MergeStatus {
        let last_merge_ts = LAST_MERGE_TS.load(Ordering::Relaxed);
        let now_ms = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .map(|d| d.as_millis() as i64)
            .unwrap_or(0);
        let ms_since_last = if last_merge_ts > 0 {
            now_ms - last_merge_ts
        } else {
            -1 // never merged
        };
        MergeStatus {
            segment_count: self.segment_count(),
            last_merge_ts,
            ms_since_last_merge: ms_since_last,
            merge_threshold: MERGE_SEGMENT_THRESHOLD,
            cooldown_ms: MERGE_COOLDOWN_MS,
        }
    }

    /// Attempt to merge segments if idle conditions are met.
    /// Returns Ok(true) if merge was triggered, Ok(false) if skipped.
    /// Merge runs in background thread - this call is non-blocking.
    pub fn optimize_if_idle(&mut self) -> Result<bool> {
        let segment_ids = self.index.searchable_segment_ids()?;
        let segment_count = segment_ids.len();

        // Check if we have enough segments to warrant a merge
        if segment_count < MERGE_SEGMENT_THRESHOLD {
            debug!(
                segments = segment_count,
                threshold = MERGE_SEGMENT_THRESHOLD,
                "Skipping merge: segment count below threshold"
            );
            return Ok(false);
        }

        // Check cooldown period
        let now_ms = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .map(|d| d.as_millis() as i64)
            .unwrap_or(0);
        let last_merge = LAST_MERGE_TS.load(Ordering::Relaxed);
        if last_merge > 0 && (now_ms - last_merge) < MERGE_COOLDOWN_MS {
            debug!(
                ms_since_last = now_ms - last_merge,
                cooldown = MERGE_COOLDOWN_MS,
                "Skipping merge: cooldown period active"
            );
            return Ok(false);
        }

        // Trigger merge - this runs asynchronously in Tantivy's merge thread pool
        info!(
            segments = segment_count,
            "Starting background segment merge"
        );

        // merge() returns a FutureResult that runs async; we drop it to let it run in background
        // The merge will complete when Tantivy's internal thread pool processes it
        let _merge_future = self.writer.merge(&segment_ids);
        LAST_MERGE_TS.store(now_ms, Ordering::Relaxed);
        info!("Segment merge initiated (running in background)");
        Ok(true)
    }

    /// Force immediate segment merge and wait for completion.
    /// Use sparingly - blocks until merge finishes.
    pub fn force_merge(&mut self) -> Result<()> {
        let segment_ids = self.index.searchable_segment_ids()?;
        if segment_ids.is_empty() {
            return Ok(());
        }
        info!(segments = segment_ids.len(), "Force merging all segments");
        let now_ms = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .map(|d| d.as_millis() as i64)
            .unwrap_or(0);

        // Start merge and wait for completion
        let merge_future = self.writer.merge(&segment_ids);
        match merge_future.wait() {
            Ok(_) => {
                LAST_MERGE_TS.store(now_ms, Ordering::Relaxed);
                info!("Force merge completed");
                Ok(())
            }
            Err(e) => {
                warn!(error = %e, "Force merge failed");
                Err(anyhow!("merge failed: {e}"))
            }
        }
    }

    pub fn add_messages(
        &mut self,
        conv: &NormalizedConversation,
        messages: &[crate::connectors::NormalizedMessage],
    ) -> Result<()> {
        for msg in messages {
            let mut d = doc! {
                self.fields.agent => conv.agent_slug.clone(),
                self.fields.source_path => conv.source_path.to_string_lossy().into_owned(),
                self.fields.msg_idx => msg.idx as u64,
                self.fields.content => msg.content.clone(),
            };
            if let Some(ws) = &conv.workspace {
                d.add_text(self.fields.workspace, ws.to_string_lossy());
            }
            if let Some(ts) = msg.created_at.or(conv.started_at) {
                d.add_i64(self.fields.created_at, ts);
            }
            if let Some(title) = &conv.title {
                d.add_text(self.fields.title, title);
                d.add_text(self.fields.title_prefix, generate_edge_ngrams(title));
            }
            d.add_text(
                self.fields.content_prefix,
                generate_edge_ngrams(&msg.content),
            );
            d.add_text(self.fields.preview, build_preview(&msg.content, 200));
            self.writer.add_document(d)?;
        }
        Ok(())
    }
}

fn generate_edge_ngrams(text: &str) -> String {
    let mut ngrams = String::with_capacity(text.len() * 2);
    // Split by non-alphanumeric characters to identify words
    for word in text.split(|c: char| !c.is_alphanumeric()) {
        let chars: Vec<char> = word.chars().collect();
        if chars.len() < 2 {
            continue;
        }
        // Generate edge ngrams of length 2..=20 (or word length)
        for len in 2..=chars.len().min(20) {
            if !ngrams.is_empty() {
                ngrams.push(' ');
            }
            ngrams.extend(chars[0..len].iter());
        }
    }
    ngrams
}

pub fn build_schema() -> Schema {
    let mut schema_builder = Schema::builder();
    let text = TextOptions::default()
        .set_indexing_options(
            TextFieldIndexing::default()
                .set_tokenizer("hyphen_normalize")
                .set_index_option(IndexRecordOption::WithFreqsAndPositions),
        )
        .set_stored();

    let text_not_stored = TextOptions::default().set_indexing_options(
        TextFieldIndexing::default()
            .set_tokenizer("hyphen_normalize")
            .set_index_option(IndexRecordOption::WithFreqsAndPositions),
    );

    // Use STRING (not TEXT) so agent slug is stored as a single non-tokenized term.
    // This ensures exact match filtering works correctly with TermQuery.
    schema_builder.add_text_field("agent", STRING | STORED);
    schema_builder.add_text_field("workspace", STRING | STORED);
    schema_builder.add_text_field("source_path", STORED);
    schema_builder.add_u64_field("msg_idx", INDEXED | STORED);
    schema_builder.add_i64_field("created_at", INDEXED | STORED | FAST);
    schema_builder.add_text_field("title", text.clone());
    schema_builder.add_text_field("content", text);
    schema_builder.add_text_field("title_prefix", text_not_stored.clone());
    schema_builder.add_text_field("content_prefix", text_not_stored);
    schema_builder.add_text_field("preview", TEXT | STORED);
    schema_builder.build()
}

pub fn fields_from_schema(schema: &Schema) -> Result<Fields> {
    let get = |name: &str| {
        schema
            .get_field(name)
            .map_err(|_| anyhow!("schema missing {name}"))
    };
    Ok(Fields {
        agent: get("agent")?,
        workspace: get("workspace")?,
        source_path: get("source_path")?,
        msg_idx: get("msg_idx")?,
        created_at: get("created_at")?,
        title: get("title")?,
        content: get("content")?,
        title_prefix: get("title_prefix")?,
        content_prefix: get("content_prefix")?,
        preview: get("preview")?,
    })
}

fn build_preview(content: &str, max_chars: usize) -> String {
    let char_count = content.chars().count();
    if char_count <= max_chars {
        return content.to_string();
    }
    let mut out = String::new();
    for ch in content.chars().take(max_chars) {
        out.push(ch);
    }
    out.push('â€¦');
    out
}

pub fn index_dir(base: &Path) -> Result<std::path::PathBuf> {
    let dir = base.join("index").join(SCHEMA_VERSION);
    std::fs::create_dir_all(&dir)?;
    Ok(dir)
}

pub fn ensure_tokenizer(index: &mut Index) {
    use tantivy::tokenizer::{LowerCaser, RemoveLongFilter, SimpleTokenizer, TextAnalyzer};
    let analyzer = TextAnalyzer::builder(SimpleTokenizer::default())
        .filter(LowerCaser)
        .filter(RemoveLongFilter::limit(40))
        .build();
    index.tokenizers().register("hyphen_normalize", analyzer);
}

// =============================================================================
// Index Corruption Handling Tests (tst.idx.corrupt)
// Tests for graceful handling of corrupted or invalid index states
// =============================================================================

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    #[test]
    fn open_or_create_handles_missing_schema_hash() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create index first
        {
            let _index = TantivyIndex::open_or_create(path).unwrap();
        }

        // Remove schema_hash.json to simulate corruption
        fs::remove_file(path.join("schema_hash.json")).unwrap();

        // Should recreate cleanly without panic
        let result = TantivyIndex::open_or_create(path);
        assert!(
            result.is_ok(),
            "Should handle missing schema_hash.json gracefully"
        );
    }

    #[test]
    fn open_or_create_handles_invalid_schema_hash() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create index first
        {
            let _index = TantivyIndex::open_or_create(path).unwrap();
        }

        // Corrupt the schema_hash.json
        fs::write(
            path.join("schema_hash.json"),
            r#"{"schema_hash":"invalid-hash"}"#,
        )
        .unwrap();

        // Should detect mismatch and rebuild
        let result = TantivyIndex::open_or_create(path);
        assert!(
            result.is_ok(),
            "Should handle invalid schema_hash gracefully"
        );
    }

    #[test]
    fn open_or_create_handles_corrupted_schema_hash_json() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create index first
        {
            let _index = TantivyIndex::open_or_create(path).unwrap();
        }

        // Write completely invalid JSON
        fs::write(path.join("schema_hash.json"), "{ invalid json {{").unwrap();

        // Should fail to read (non-JSON) but rebuild successfully
        let result = TantivyIndex::open_or_create(path);
        // Reading invalid JSON will fail but rebuild should happen
        assert!(
            result.is_ok() || result.is_err(),
            "Should not panic on corrupted schema_hash.json"
        );
    }

    #[test]
    fn open_or_create_handles_empty_directory() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Open on empty directory - should create new
        let result = TantivyIndex::open_or_create(path);
        assert!(result.is_ok(), "Should create new index in empty directory");
    }

    #[test]
    fn open_or_create_handles_missing_meta_json() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create valid schema_hash but no meta.json
        fs::create_dir_all(path).unwrap();
        fs::write(
            path.join("schema_hash.json"),
            format!(r#"{{"schema_hash":"{SCHEMA_HASH}"}}"#),
        )
        .unwrap();

        // Should create new index (meta.json missing triggers create)
        let result = TantivyIndex::open_or_create(path);
        assert!(
            result.is_ok(),
            "Should create new index when meta.json missing"
        );
    }

    #[test]
    fn open_or_create_handles_corrupted_meta_json() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create index first
        {
            let _index = TantivyIndex::open_or_create(path).unwrap();
        }

        // Corrupt meta.json (Tantivy's index metadata file)
        let meta_path = path.join("meta.json");
        if meta_path.exists() {
            fs::write(&meta_path, "corrupted meta content").unwrap();
        }

        // Should detect corruption and rebuild (schema hash won't match or open fails)
        // Note: This may fail on open, but should not panic
        let result = TantivyIndex::open_or_create(path);
        // Accept either success (rebuild) or error (corruption detected)
        assert!(
            result.is_ok() || result.is_err(),
            "Should not panic on corrupted meta.json"
        );
    }

    #[test]
    fn open_or_create_handles_truncated_segment_file() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create index and add some data
        {
            let mut index = TantivyIndex::open_or_create(path).unwrap();
            // Create a simple doc to generate a segment
            let doc = doc! {
                index.fields.agent => "test_agent",
                index.fields.source_path => "/test/path",
                index.fields.msg_idx => 0u64,
                index.fields.content => "test content for segment",
            };
            index.writer.add_document(doc).unwrap();
            index.commit().unwrap();
        }

        // Find and truncate any .store or .idx files
        for entry in fs::read_dir(path).unwrap() {
            let entry = entry.unwrap();
            let name = entry.file_name();
            let name_str = name.to_string_lossy();
            if name_str.ends_with(".store") || name_str.ends_with(".idx") {
                // Truncate the file
                let file = fs::OpenOptions::new()
                    .write(true)
                    .truncate(true)
                    .open(entry.path())
                    .unwrap();
                file.set_len(10).unwrap(); // Leave only 10 bytes
                break;
            }
        }

        // Should handle truncated segment gracefully
        let result = TantivyIndex::open_or_create(path);
        // Accept either success (recreate) or error (detected corruption) - no panic
        assert!(
            result.is_ok() || result.is_err(),
            "Should not panic on truncated segment file"
        );
    }

    #[test]
    fn open_or_create_roundtrip_add_and_search() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create, add, and verify data survives
        {
            let mut index = TantivyIndex::open_or_create(path).unwrap();
            let doc = doc! {
                index.fields.agent => "test_agent",
                index.fields.source_path => "/test/path",
                index.fields.msg_idx => 0u64,
                index.fields.content => "hello world test content",
            };
            index.writer.add_document(doc).unwrap();
            index.commit().unwrap();
        }

        // Reopen and verify
        {
            let index = TantivyIndex::open_or_create(path).unwrap();
            let reader = index.reader().unwrap();
            let searcher = reader.searcher();

            // Should have at least 1 document
            assert!(
                searcher.num_docs() >= 1,
                "Should have at least 1 document after roundtrip"
            );
        }
    }

    #[test]
    fn open_or_create_rebuild_on_schema_mismatch() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create index first
        {
            let _index = TantivyIndex::open_or_create(path).unwrap();
        }

        // Write an old/different schema hash
        fs::write(
            path.join("schema_hash.json"),
            r#"{"schema_hash":"old-schema-v1"}"#,
        )
        .unwrap();

        // Should rebuild (delete old and create new)
        let result = TantivyIndex::open_or_create(path);
        assert!(result.is_ok(), "Should rebuild index on schema mismatch");

        // Verify the new schema hash is written
        let hash_content = fs::read_to_string(path.join("schema_hash.json")).unwrap();
        assert!(
            hash_content.contains(SCHEMA_HASH),
            "Should write correct schema hash after rebuild"
        );
    }

    #[test]
    fn build_schema_returns_valid_schema() {
        let schema = build_schema();

        // Verify all required fields exist
        assert!(schema.get_field("agent").is_ok());
        assert!(schema.get_field("workspace").is_ok());
        assert!(schema.get_field("source_path").is_ok());
        assert!(schema.get_field("msg_idx").is_ok());
        assert!(schema.get_field("created_at").is_ok());
        assert!(schema.get_field("title").is_ok());
        assert!(schema.get_field("content").is_ok());
        assert!(schema.get_field("title_prefix").is_ok());
        assert!(schema.get_field("content_prefix").is_ok());
        assert!(schema.get_field("preview").is_ok());
    }

    #[test]
    fn fields_from_schema_extracts_all_fields() {
        let schema = build_schema();
        let fields = fields_from_schema(&schema).unwrap();

        // Verify fields are valid (non-panicking access)
        let _ = fields.agent;
        let _ = fields.workspace;
        let _ = fields.source_path;
        let _ = fields.msg_idx;
        let _ = fields.created_at;
        let _ = fields.title;
        let _ = fields.content;
        let _ = fields.title_prefix;
        let _ = fields.content_prefix;
        let _ = fields.preview;
    }

    #[test]
    fn generate_edge_ngrams_produces_prefixes() {
        let result = generate_edge_ngrams("hello");
        // Should generate ngrams: "he", "hel", "hell", "hello"
        assert!(result.contains("he"));
        assert!(result.contains("hel"));
        assert!(result.contains("hell"));
        assert!(result.contains("hello"));
    }

    #[test]
    fn generate_edge_ngrams_handles_empty_string() {
        let result = generate_edge_ngrams("");
        assert!(result.is_empty());
    }

    #[test]
    fn generate_edge_ngrams_handles_short_string() {
        // Single char words are skipped (len < 2)
        let result = generate_edge_ngrams("a");
        assert!(result.is_empty());

        // Two char word generates just "ab"
        let result = generate_edge_ngrams("ab");
        assert_eq!(result, "ab");
    }

    #[test]
    fn generate_edge_ngrams_handles_multiple_words() {
        let result = generate_edge_ngrams("hello world");
        // Should contain ngrams from both words
        assert!(result.contains("he"));
        assert!(result.contains("wo"));
        assert!(result.contains("world"));
    }

    #[test]
    fn merge_status_should_merge_logic() {
        let status = MergeStatus {
            segment_count: 5,
            last_merge_ts: 0,
            ms_since_last_merge: -1, // never merged
            merge_threshold: 4,
            cooldown_ms: 300_000,
        };
        assert!(
            status.should_merge(),
            "Should merge when never merged and above threshold"
        );

        let status_below_threshold = MergeStatus {
            segment_count: 2,
            last_merge_ts: 0,
            ms_since_last_merge: -1,
            merge_threshold: 4,
            cooldown_ms: 300_000,
        };
        assert!(
            !status_below_threshold.should_merge(),
            "Should not merge when below threshold"
        );

        let status_in_cooldown = MergeStatus {
            segment_count: 5,
            last_merge_ts: 1000,
            ms_since_last_merge: 1000, // Only 1 second since last
            merge_threshold: 4,
            cooldown_ms: 300_000, // 5 minute cooldown
        };
        assert!(
            !status_in_cooldown.should_merge(),
            "Should not merge during cooldown"
        );

        let status_after_cooldown = MergeStatus {
            segment_count: 5,
            last_merge_ts: 1000,
            ms_since_last_merge: 400_000, // 6+ minutes since last
            merge_threshold: 4,
            cooldown_ms: 300_000,
        };
        assert!(
            status_after_cooldown.should_merge(),
            "Should merge after cooldown expires"
        );
    }

    #[test]
    fn index_dir_creates_versioned_path() {
        let dir = TempDir::new().unwrap();
        let result = index_dir(dir.path()).unwrap();

        assert!(result.ends_with(format!("index/{}", SCHEMA_VERSION)));
        assert!(result.exists());
    }

    // =============================================================================
    // Full Index Rebuild Tests (tst.idx.rebuild)
    // Tests for complete index rebuild scenarios
    // =============================================================================

    #[test]
    fn rebuild_from_empty_directory() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Should create index from scratch in empty directory
        let result = TantivyIndex::open_or_create(path);
        assert!(result.is_ok(), "Should create index from empty directory");

        // Verify schema hash is written
        let hash_path = path.join("schema_hash.json");
        assert!(hash_path.exists(), "Should write schema_hash.json");

        let hash_content = fs::read_to_string(hash_path).unwrap();
        assert!(
            hash_content.contains(SCHEMA_HASH),
            "Should contain current schema hash"
        );
    }

    #[test]
    fn rebuild_creates_meta_json() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        let index = TantivyIndex::open_or_create(path).unwrap();
        drop(index);

        // Tantivy should have created meta.json
        let meta_path = path.join("meta.json");
        assert!(meta_path.exists(), "Should create meta.json");
    }

    #[test]
    fn rebuild_doc_count_matches_added_documents() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Add multiple documents
        {
            let mut index = TantivyIndex::open_or_create(path).unwrap();

            for i in 0..5 {
                let doc = doc! {
                    index.fields.agent => "test_agent",
                    index.fields.source_path => format!("/test/path/{}", i),
                    index.fields.msg_idx => i as u64,
                    index.fields.content => format!("content {}", i),
                };
                index.writer.add_document(doc).unwrap();
            }
            index.commit().unwrap();
        }

        // Verify doc count
        {
            let index = TantivyIndex::open_or_create(path).unwrap();
            let reader = index.reader().unwrap();
            let searcher = reader.searcher();

            assert_eq!(searcher.num_docs(), 5, "Should have exactly 5 documents");
        }
    }

    #[test]
    fn rebuild_delete_all_clears_index() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Add documents
        {
            let mut index = TantivyIndex::open_or_create(path).unwrap();
            for i in 0..3 {
                let doc = doc! {
                    index.fields.agent => "test_agent",
                    index.fields.source_path => format!("/test/path/{}", i),
                    index.fields.msg_idx => i as u64,
                    index.fields.content => format!("content {}", i),
                };
                index.writer.add_document(doc).unwrap();
            }
            index.commit().unwrap();

            // Delete all and commit
            index.delete_all().unwrap();
            index.commit().unwrap();
        }

        // Verify empty
        {
            let index = TantivyIndex::open_or_create(path).unwrap();
            let reader = index.reader().unwrap();
            let searcher = reader.searcher();

            assert_eq!(
                searcher.num_docs(),
                0,
                "Should have 0 documents after delete_all"
            );
        }
    }

    #[test]
    fn rebuild_force_via_schema_change() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create and add documents
        {
            let mut index = TantivyIndex::open_or_create(path).unwrap();
            let doc = doc! {
                index.fields.agent => "test_agent",
                index.fields.source_path => "/test/path",
                index.fields.msg_idx => 0u64,
                index.fields.content => "original content",
            };
            index.writer.add_document(doc).unwrap();
            index.commit().unwrap();
        }

        // Simulate forcing rebuild by changing schema hash
        fs::write(
            path.join("schema_hash.json"),
            r#"{"schema_hash":"force-rebuild-v0"}"#,
        )
        .unwrap();

        // Reopen - should rebuild (losing old data)
        {
            let index = TantivyIndex::open_or_create(path).unwrap();
            let reader = index.reader().unwrap();
            let searcher = reader.searcher();

            // After force rebuild, index is empty
            assert_eq!(
                searcher.num_docs(),
                0,
                "Should have 0 documents after force rebuild"
            );
        }
    }

    #[test]
    fn rebuild_preserves_data_when_schema_matches() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create and add documents
        {
            let mut index = TantivyIndex::open_or_create(path).unwrap();
            let doc = doc! {
                index.fields.agent => "preserved_agent",
                index.fields.source_path => "/preserved/path",
                index.fields.msg_idx => 42u64,
                index.fields.content => "preserved content",
            };
            index.writer.add_document(doc).unwrap();
            index.commit().unwrap();
        }

        // Reopen without schema change - should preserve data
        {
            let index = TantivyIndex::open_or_create(path).unwrap();
            let reader = index.reader().unwrap();
            let searcher = reader.searcher();

            assert_eq!(
                searcher.num_docs(),
                1,
                "Should preserve 1 document when schema matches"
            );
        }
    }

    #[test]
    fn rebuild_all_fields_searchable_after_add() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        let mut index = TantivyIndex::open_or_create(path).unwrap();

        // Add document with all fields
        let doc = doc! {
            index.fields.agent => "claude_code",
            index.fields.workspace => "/workspace/project",
            index.fields.source_path => "/path/to/session.jsonl",
            index.fields.msg_idx => 0u64,
            index.fields.created_at => 1700000000i64,
            index.fields.title => "Test Session Title",
            index.fields.content => "This is the message content",
            index.fields.title_prefix => generate_edge_ngrams("Test Session Title"),
            index.fields.content_prefix => generate_edge_ngrams("This is the message content"),
            index.fields.preview => "Preview text",
        };
        index.writer.add_document(doc).unwrap();
        index.commit().unwrap();

        let reader = index.reader().unwrap();
        let searcher = reader.searcher();

        // Verify document is indexed
        assert!(
            searcher.num_docs() >= 1,
            "Document should be searchable after add"
        );
    }

    #[test]
    fn rebuild_schema_version_consistency() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create index
        {
            let _index = TantivyIndex::open_or_create(path).unwrap();
        }

        // Read and verify schema hash format
        let hash_content = fs::read_to_string(path.join("schema_hash.json")).unwrap();
        let expected = format!(r#"{{"schema_hash":"{}"}}"#, SCHEMA_HASH);
        assert_eq!(hash_content, expected, "Schema hash format should match");
    }

    #[test]
    fn rebuild_commit_creates_searchable_state() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        let mut index = TantivyIndex::open_or_create(path).unwrap();

        // Add without commit - not searchable in new reader
        let doc = doc! {
            index.fields.agent => "test",
            index.fields.source_path => "/test",
            index.fields.msg_idx => 0u64,
            index.fields.content => "before commit",
        };
        index.writer.add_document(doc).unwrap();

        // After commit - searchable
        index.commit().unwrap();

        let reader = index.reader().unwrap();
        let searcher = reader.searcher();
        assert!(
            searcher.num_docs() >= 1,
            "Document should be searchable after commit"
        );
    }

    #[test]
    fn rebuild_multiple_commits_accumulate() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        let mut index = TantivyIndex::open_or_create(path).unwrap();

        // First batch
        let doc1 = doc! {
            index.fields.agent => "agent1",
            index.fields.source_path => "/path1",
            index.fields.msg_idx => 0u64,
            index.fields.content => "first batch",
        };
        index.writer.add_document(doc1).unwrap();
        index.commit().unwrap();

        // Second batch
        let doc2 = doc! {
            index.fields.agent => "agent2",
            index.fields.source_path => "/path2",
            index.fields.msg_idx => 0u64,
            index.fields.content => "second batch",
        };
        index.writer.add_document(doc2).unwrap();
        index.commit().unwrap();

        // Third batch
        let doc3 = doc! {
            index.fields.agent => "agent3",
            index.fields.source_path => "/path3",
            index.fields.msg_idx => 0u64,
            index.fields.content => "third batch",
        };
        index.writer.add_document(doc3).unwrap();
        index.commit().unwrap();

        let reader = index.reader().unwrap();
        let searcher = reader.searcher();
        assert_eq!(
            searcher.num_docs(),
            3,
            "Should have 3 documents after 3 commits"
        );
    }

    #[test]
    fn rebuild_empty_index_has_zero_docs() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        let index = TantivyIndex::open_or_create(path).unwrap();
        let reader = index.reader().unwrap();
        let searcher = reader.searcher();

        assert_eq!(searcher.num_docs(), 0, "New index should have 0 documents");
    }

    #[test]
    fn rebuild_can_reopen_after_close() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        // Create, add, close
        {
            let mut index = TantivyIndex::open_or_create(path).unwrap();
            let doc = doc! {
                index.fields.agent => "test",
                index.fields.source_path => "/test",
                index.fields.msg_idx => 0u64,
                index.fields.content => "content",
            };
            index.writer.add_document(doc).unwrap();
            index.commit().unwrap();
        }

        // Reopen
        let result = TantivyIndex::open_or_create(path);
        assert!(result.is_ok(), "Should be able to reopen after close");

        let index = result.unwrap();
        let reader = index.reader().unwrap();
        let searcher = reader.searcher();
        assert_eq!(searcher.num_docs(), 1, "Data should persist after reopen");
    }

    #[test]
    fn rebuild_handles_large_batch() {
        let dir = TempDir::new().unwrap();
        let path = dir.path();

        let mut index = TantivyIndex::open_or_create(path).unwrap();

        // Add 100 documents
        for i in 0..100 {
            let doc = doc! {
                index.fields.agent => "batch_agent",
                index.fields.source_path => format!("/batch/path/{}", i),
                index.fields.msg_idx => i as u64,
                index.fields.content => format!("batch content number {}", i),
            };
            index.writer.add_document(doc).unwrap();
        }
        index.commit().unwrap();

        let reader = index.reader().unwrap();
        let searcher = reader.searcher();
        assert_eq!(
            searcher.num_docs(),
            100,
            "Should have 100 documents after large batch"
        );
    }
}
